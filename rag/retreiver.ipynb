{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': 'speech.txt'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader= TextLoader(\"speech.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Detection of Crime Scene Objects using Deep \\nLearning Techniques \\n \\nNandhini T J  K Thinakaran  \\nDepartment of Computer Science and Engineering  Department of Computer Science and Engineering  \\nSaveetha School of Engineering  Saveetha School of Engineering  \\nSaveetha Institute of Medical and Technical Science ,  \\nChennai, India . Saveetha Institute of Medical and Technical Science,  \\nChennai, India.  \\nnandhinitj67@gmail.com  thinakarank.sse@saveetha.com  \\n \\nAbstract-  Research on the detection of objects at crime scenes has \\nflourished in the last two decades. Researchers have been \\nconcentrating on color pictures, where lighting is a crucial \\ncomponent, since this is one of the most pressing issues in \\ncomputer vision, with applications spanning surveillance, security, \\nmedicine, and more. However, night time monitoring is crucial \\nsince most security problems cannot be seen by the naked eye. \\nThat's why it's crucial to record a dark scene and identify the \\nthings at a crime scene. Even when its dark out, infrared cameras \\nare indispensable. Both military and civilian sectors will benefit \\nfrom the use of such methods for night time navigation. On the \\nother hand, IR photographs have issues with poor resolution, \\nlighting effects, and other similar issues. Surveillance cameras \\nwith infrared (IR) imaging capabilities have been the focus of \\nmuch study and development in recent years. This research work \\nhas attempted to offer a good model for object recognition by \\nusing IR images obtained from crime scenes using Deep Learning. \\nThe model is tested in many scenarios including a central \\nprocessing unit (CPU), Google COLAB, and graphics processing \\nunit (GPU), and its performance is also tabulated. \\n \\nKeywords: Object detection, Deep Learning, COLA B, CNN, Crime \\nscene. \\n \\nI. INTRODUCTION \\nIn crime scene object detection, a wide range of computer \\nvision applications, including autonomous driving, cutting-\\nedge driving assistance systems, robotic visions, augmented \\nreality, etc., make object identification a significant and active \\narea of research. The primary goal of object detection is to \\nlocate and categorize particular items in still photographs and \\nmoving pictures.[1][2]. The process of focusing on an object \\ninvolved in the vision process, such as visual tracking, human \\nre-identification, and semantic segmentation, is typically seen \\nas an essential step.[3] The semantic object detection technique \\nmakes use of several geometric patterns as proof to spot \\nintriguing objects in pictures or movies.[4] The patterns of \\nforms that display a comparable category of objects are used to \\ntrain the object recognition models to distinguish between \\ndistinct categories. However, because the characteristics of \\nbasic item forms, object positions, and angles of view vary \\nwidely, it is challenging for a system to reliably identify every \\nappearance of an object.[5] Multiple object detection uses \\nsimilarities between the succession of photos or videos \\ndetermine the movement of things. Target objects are first \\nidentified in multiple object detection and the technique is then \\nfollowed to assess the itinerary of the items using the results of \\nthe detection. The journey of many objects is formed by \\nsemantic object detection with detection, which utilizes \\nassociated data from the existing track and fresh identification \\nfrom each frame.[6] Thus, a sequence of detections with distinct identities is produced as a result of data association. \\nRecognition of salient objects might be difficult when they \\nhave similar appearances. The moving objects are indicated for \\ndifferentiating and monitoring the different objects in this \\nsituation.[7] When using a single moving camera, the \\nmovement of the global camera, which cannot be seen, \\ncontaminates the discernible indications of motion. Due to the \\ndecreased characteristics of images, such as blurriness in \\nmotion and defocus on films, which results in inconsistent \\ncategorization for comparable objects, object detection \\npresented a challenge. Convolutional Neural Network (CNN), \\nfaster region-based CNN, spatial pyramid pooling network, \\nregion-based Fully CNN, You Only Look Once (YOLO), and \\nFeature Pyramid Network (FPN) are some of the deep learning \\nmodels that have been used to build the semantic object \\nidentification approach.[8] \\nThis study develops an algorithm to categorise different crime \\nscene photographs and to recognise different things in them. A \\nvideo that is composed of photographs from crime scenes is \\ntaken into account and divided into frames. The frame rate \\nrange for video is between 45 to 120 frames per second, or \\n7200 pictures per minute[9][10]. When processing any video, \\nthis step is frequently taken. A seven-layered convolutional \\nneural network is used to run the video after receiving the \\nphotos, and it typically recognises the images based on the \\ntrained images[11]. \\nThe current algorithms effectively identify items in some \\nlabeled photos, but they needed to be given positions, classes, \\nand background distributions. However, when the objects were \\nmanually annotated, the assignment process was tedious and \\ntime-consuming. The handcrafted features of the old sliding \\nwindow object identification approach had limitations that \\nmade it difficult to reliably recognize the items. Additionally, \\nCNN succeeded in object detection by outperforming the \\nconventional method. But because of difficult conditions \\nincluding object occlusion, more fluctuation in object scale, \\nand dim lighting, the CNN detector was not able to attain \\nacceptable accuracy.  \\nII. RELATED WORKS \\nThis section reviews and describes existing object detection \\ntechniques, as well as their benefits and drawbacks . Sucheng \\nRen developed a new video crime scene object detection \\n(VCSOD) method that uses a triple excitation mechanism. To \\nsolve the changing and contradicting Spatio-Temporal feature \\ndifficulties during the training phase, spatial and temporal \\nexcitations are offered. Additionally, semi-curriculum learning Proceedings of the International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT 2023)\\nIEEE Xplore Part Number: CFP23CV1-ART; ISBN: 978-1-6654-7451-1\\n978-1-6654-7451-1/23/$31.00 ©2023 IEEE 3572023 International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT) | 978-1-6654-7451-1/23/$31.00 ©2023 IEEE | DOI: 10.1109/IDCIoT56793.2023.10053440\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 18,2024 at 14:27:08 UTC from IEEE Xplore.  Restrictions apply. \", metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 0}),\n",
       " Document(page_content='is employed in training phase to initially reduce the task \\ndifficulty and get a higher level of convergence. In addition, we \\nsuggest the network\\'s first online excitation during testing so \\nthat it can keep improving the saliency result by excitation \\nwhile using the saliency map  generated by the network. \\nExtensive testing shows that our outcomes perform better than \\nthose of any competitors. [12] \\nXuebin Qin developed an innovative deep network called U2 - \\nNet for the detection of salient objects. The m ain structure of \\nour U2 -Net is a two -level nested U -structure. No of the \\nresolution, the network can gather greater local and global \\ninformation from both shallow and deep layers because to the \\nlayered U -structure and our newly developed RSU blocks. [13] \\nFu et.al. developed a region -based Convolutional Neural \\nNetwork Framework for arbitrary and multi -scale item \\nrecognition in remote sensing pictures The feature fusion \\narchitecture was developed in order to extract detection \\ncharacteristics based on the Region of Interest (RoI). To \\nacquire the precise position of arbitrarily oriented objects, the \\nOriented Region Proposal Network (RPN -O) was constructed, \\nand RoI po oling was utilised to avoid orientation changes. \\nBecause the established CNN architecture proved resistant to \\nobjects in remote sensing images, anchors with additional \\nscales and angles were added to aid RPN in object \\ndetection [14]. However, the created feature fusion method was \\nunable to identify comparable appearances and suffered from \\nidentifying the backdrop of images, which hampered object \\nidentification performance.  \\nCai and Vasconcelos created a cascade Region -based CNN \\napproach for increasing the quality of object detection and \\nsegmentation. Inference uses the cascade to remove \\nmismatched detectors and improve hypothesis quality. The \\nresampling strategy increases hypothesis quality greatly by \\ngiving a positive training set with simil ar sizes for each \\ndetector and reducing overfitting. The created method, \\nhowever, maximised the diversity of samples utilised to \\nforecast the masked object because the segmentation procedure \\nwas a patch -based operation with a larger number of highly \\noverla pping instances. [15] \\nChen et al. propose leveraging reverse attention in the top -\\ndown pathway to  steer residual saliency learning, which leads \\nthe network to uncover complementary object regions and \\ndetails. However, the methods mentioned above use just \\nindividual resolution features in each decoder unit, which is \\ninsufficient for dealing with compli cated and diverse scale \\nchallenges. [16] \\nMany recent papers in the literature use deep convolutional  \\nneural networks (e.g., AlexNet, GoogleNet, and VGG -Net) to \\ndetect and locate objects with class -specific bounding boxes. A \\nCNN is typically made up of several convolution layers, \\nfollowed by ReLU (Rectified Linear Units), pooling layers, and \\nfully connect ed layers. The activations produced by a CNN\\'s \\nfinal layers can be used as a descriptor for object detection and \\nclassification.  \\nThe Faster R -CNN technique is used in a real -time system for \\ncrime scene evidence analysis that can find objects in an indoor \\nenvironment. For object detection, the suggested system makes \\nuse of the Region Proposal Network and VGG -16 network. Compared to the current models, the suggested architecture \\nprovides a better level of accuracy.  \\nIII. PROPOSED CNN ARCHITECTURE  \\n \\nThe suggested CNN  architecture is displayed in Fig. 1 below. \\nAn infrared image measuring 640 by 480 pixels serves as the \\ninput. The architecture is made with the knowledge that the \\nimage is vulnerable to illumination, low resolution, and other \\ninfluences, making it challen ging for the image to effectively \\nprocess and recognize the item. Therefore, this design consists \\nof Seven Convolution layers with Max -pooling, One Flatten, \\nand the SoftMax activation function. 32 filters are used in the \\nfirst convolution layer, and 100 fi lters, each measuring 3 x 3, \\nare used in the remaining six hidden layers. Max -pooling is \\nalso carried out with a scale of 2 x 2 at every layer. Following \\nthat, the convolutional layers are flattened and normalized \\nusing the SoftMax step. All layers employ the activation \\nfunction \"ReLu ” and the output range is varying  from 0 to \\ninfinity.  The shear and stride values are both 1.  The activation \\nfunction of ReLu is given by the equation 1.  \\nf(x) = max(0,x)                 (1)  \\nThree different environments were used for the \\nexperimentation. The entire work was written in Keras and \\ntested with two different datasets of 189 images and 147 \\nimages, which were later tested on GPU with 1820 images. The \\ndatasets under consideration are FLIR, and preliminary testing \\nwas performed on a system equipped with a Core i5, 8 GB \\nRAM, and a 1 TB HDD. Later, experiments were carried out \\non a NIVIDIA DX -1 GPU with 128 GB RAM and a speed of 1 \\nTesla. The experiment was carried out with a dataset of 14 7 \\nimages and 189 images with 7 different classes and 21 different \\nclasses images and 27 images per class, respectively  \\n \\n        \\nFig. 1. Convolution Neural Network Architecture  \\n \\nThe proposed model is evaluated based on various evaluation \\nmetrics such as Pr ecision, Recall, F -Score, and Accuracy, \\nwhich are defined as,  \\n1) Precision = TP / (FP + TP)  \\n2) Recall = TP / (FN + TP)  \\n3) F1-Score = (2 * Recall * Precision) / (Recall + \\nPrecision)  \\n4) Accuracy = (TP + TN) / (TP + FP + FN + TN)  \\nProceedings of the International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT 2023)\\nIEEE Xplore Part Number: CFP23CV1-ART; ISBN: 978-1-6654-7451-1\\n978-1-6654-7451-1/23/$31.00 ©2023 IEEE 358\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 18,2024 at 14:27:08 UTC from IEEE Xplore.  Restrictions apply. ', metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 1}),\n",
       " Document(page_content='TP = True Positives, TN = True Negat ives, FP = False \\nPositives, FN = False Negatives  \\n \\nA. TRAINING DATASET  \\nNow that we have the data set, we need to pre -process it a little \\nand label each of the images that were provided during training \\nthe data set. To do so, we can see that the names of each image \\nin the training data set begin with \"knife\" or \"gun,\" which we \\nwill exploit, and then we will use one hot encoder for the \\nmachine to understand the labels (Knife [1, 0] or gun [0, 1]).  \\nThe datasets which have been used in this analysis is divided \\ninto two groups which is training and testing dataset in which \\n75% is taken for training and 25% for testing.  \\n \\n \\n \\nFig.. 2. Dataset for Training  \\nB. TESTING DATASET  \\n  \\n  \\nFig.. 3. Dataset for Testing  \\n \\n \\n \\n \\n \\n \\n \\n \\n IV. EXPERIMENTAL ANALYSIS  \\nTABLE 1. CONFUSION MATRIX FOR 147 IMAGES EACH CLASS WITH \\n21 IMAGES  \\n            PREDICTIO N  \\nobjects  Knife  Cell  \\nphone  Car Animal  Gun  Blood  Currency  \\nKnife  27 0 0 0 0 0 0 \\nCell  \\nphone  0 26 0 0 0 0 0 \\nCar 1 0 26 0 0 0 0 \\nAnimal  0 0 0 26 0 0 0 \\nGun  0 0 0 0 21 0 0 \\nBlood  0 1 0 0 0 26 0 \\nCurrency  0 0 0 0 1 0 28 \\n \\nTABLE II. CONFUSION MATRIX FOR 189 IMAGES EACH CLASS WITH \\n27 IMAGES  \\nPREDICTIO N  \\nobjects  Knife  Cell  \\nphone  Car Animal  Gun Blood  Currency  \\nKnife  20  0  0  0  0  0  0  \\nCell \\nphone  0  20  0  0  0  0  0  \\nCar 1  0  21  0  0  0  0  \\nAnimal  0  0  0  21  0  0  0  \\nGun  0  1  0  0  20  0  0  \\nBlood  0  0  0  0  0  20 0  \\n \\nCurrency  0  1  0  0  1  0  21  \\n \\nTABLE III. ACCURACY FOR EACH CLASS FOR 147 IMAGES  \\nO bject  Accuracy  \\nKNIFE  0.998  \\nCELLPHONE  0.930  \\nCAR  0.965  \\nANIMALS  0.964  \\nGUN  0.972  \\nBLOOD  0.975  \\nCURRENCY  0.932  \\n \\nProceedings of the International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT 2023)\\nIEEE Xplore Part Number: CFP23CV1-ART; ISBN: 978-1-6654-7451-1\\n978-1-6654-7451-1/23/$31.00 ©2023 IEEE 359\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 18,2024 at 14:27:08 UTC from IEEE Xplore.  Restrictions apply. ', metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 2}),\n",
       " Document(page_content=' \\nFig.. 4. Accuracy for 147 Images in Each Class with 21 Images  \\nThe results are reported in the above table based on the \\naccuracy discovered through testing on 147 photos divided into \\n7 classes, where the model accurately predicted with high \\naccuracy for gun knife and blood.  The analysis is carried out by \\nusing the google colab for predicting the accuracy of the data \\nsets. The accuracy of the datasets is achived by increasing the \\ntraining datasets than the testing datasets further the accuracy \\ncan also be improved if the amount  of data in the training \\ndataset is increased.  \\n \\nThe following table provides the relevant confusion \\nmatrix for seven classes along with their TP (True Positives), \\nTN (True Negative s), FP (False Positives), and FN (False \\nNegatives).  \\n \\nTABLE IV. EVALUATION METRICS FOR 147 IMAGES IN EACH CLASS \\nWITH 21 IMAGES  \\nExperimental  Sensitivity  Specificity  Precision  F-\\nScore  Epochs  \\n1 0.89 1 1 0.96 \\n2 0.88 0.92 1 0.98 \\n3 0.94 1 1 0.93 \\n4 0.93 1 1 1 \\n5 0.96 1 1 1 \\n6 0.95 0.96 1 1 \\n7 1 0.92 1 0.95 \\n \\n  \\nFig.5. Evaluation Metrics for 147 Images In Each Class with 21 Images  \\nTABLE V. ACCURACY FOR EACH CLASS FOR 189 IMAGES  \\nObject  Accuracy  \\nKNIFE  0.987  \\nCELLPHONE  0.857  \\nCAR  0.875  \\nANIMALS  0.863  \\nGUN  0.985  \\nBLOOD  0.993  \\nCURRENCY  0.901  \\n \\n \\nFig. 6. Accuracy for 189 Images Each Class with 27 Images  \\n \\n \\n \\n \\n \\n 0.750.80.850.90.9511.05SCORE  ACCURACY  \\n0.820.840.860.880.90.920.940.960.9811.02\\n1 2 3 4 5 6 7\\nE POCHS  \\nSensitivity Specificity Precision F-Score\\n0.750.80.850.90.9511.05SCORE  ACCURACY  Proceedings of the International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT 2023)\\nIEEE Xplore Part Number: CFP23CV1-ART; ISBN: 978-1-6654-7451-1\\n978-1-6654-7451-1/23/$31.00 ©2023 IEEE 360\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 18,2024 at 14:27:08 UTC from IEEE Xplore.  Restrictions apply. ', metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 3}),\n",
       " Document(page_content=\"TABLE VI. EVALUATION METRICS FOR 189 IMAGES IN EACH CLASS \\nWITH 27 IMAGES  \\nExperimental  \\nEpochs  Sensitivity  Specificity  Precision  F-\\nScore  \\n1 0.93 0.8 0.94 0.93 \\n2 0.87 0.87 0.82 0.86 \\n3 1 0.97 0.87 0.87 \\n4 1 0.97 0.94 0.98 \\n5 0.94 0.93 0.93 0.98 \\n6 0.96 0.97 0.93 0.98 \\n7 1 0.87 0.87 1 \\n \\n \\nFig.7. Evaluation Metrics for 189 Images in Each Class with 27 Images  \\nTABLE VII. EXECUTION TIME FOR THREE ENVIRONMENTS  \\n#Images  CPU(Mins)  COLAB  (Mins)  GPU  \\n(Mins)  \\n147 25 19 3.8 \\n189 33 22 3.8 \\n \\n \\nFig. 8. Execution Time for three environments  \\nV. CONCLUSION  \\nThe development of crime scene photographs will facilitate \\ntheir use in numerous security and surveillance applications. \\nAccording to the experimental results, the suggested CNN \\narchitecture outperforms in terms of accuracy, and using GPUs \\nwould cut processing time by 90%. As a result, the suggested model produced remarkably precise findings. However, by \\nrecognizing and iden tifying the object, there is still room for \\ndevelopment. Additionally, it's crucial to comprehend the \\nscene. The same effort can be expanded to find undersea \\nobjects that will be helpful for defence  \\n \\nREFERENCE S \\n[1] A. Bathija, “Visual Object Detection and Tracking using YOLO and \\nSORT,” Int. J. Eng. Res. Technol. , vol. 8, no. 11, pp. 705 –708, 2019, \\n[Online]. Available: https://www.ijert.org  \\n[2] M. Tiwari and R. Singhai, “A Review of Detection and Tracking of \\nObjec t from Image and Video Sequences,” Int. J. Comput. Intell. \\nRes., vol. 13, no. 5, pp. 745 –765, 2017, [Online]. Available: \\nhttp://www.ripublication.com  \\n[3] T. Bergs, C. Holst, P. Gupta, and T. Augspurger, “Digital image \\nprocessing with deep learning for auto mated cutting tool wear \\ndetection,” Procedia Manuf. , vol. 48, pp. 947 –958, 2020, doi: \\n10.1016/j.promfg.2020.05.134.  \\n[4] J. Zhu, Z. Wang, S. Wang, and S. Chen, “Moving object detection \\nbased on background compensation and deep learning,” Symmetry \\n(Basel). , vol. 12, no. 12, pp. 1 –17, 2020, doi: 10.3390/sym12121965.  \\n[5] S. Ren, K. He, and R. Girshick, “Faster R -CNN\\u202f: Towards Real -Time \\nObject Detection with Region Proposal Networks,” pp. 1 –10. \\n[6] S. T. Naurin, A. Saha, K. Akter, and S. Ahmed, “A Proposed \\nArchi tecture to Suspect and Trace Criminal Activity Using \\nSurveillance Cameras,” no. June, pp. 5 –7, 2020.  \\n[7] L. E. van Dyck, R. Kwitt, S. J. Denzler, and W. R. Gruber, \\n“Comparing Object Recognition in Humans and Deep Convolutional \\nNeural Networks —An Eye Tracki ng Study,” Front. Neurosci. , vol. \\n15, no. October, pp. 1 –15, 2021, doi: 10.3389/fnins.2021.750639.  \\n[8] G. Kishore, G. Gnanasundar, and S. Harikrishnan, “A Decisive \\nObject Detection using Deep Learning Techniques,” Int. J. Innov. \\nTechnol. Explor. Eng. , vol.  9, no. 1S, pp. 414 –417, 2019, doi: \\n10.35940/ijitee.a1082.1191s19.  \\n[9] S. A. T, S. R, V. R, and K. T, “Flying Object Detection and \\nClassification using Deep Neural Networks,” Int. J. Adv. Eng. Res. \\nSci., vol. 6, no. 2, pp. 180 –183, 2019, doi: 10.22161/ijae rs.6.2.23.  \\n[10] J. I.-Z. Chen and J. -T. Chang, “Applying a 6 -axis Mechanical Arm \\nCombine with Computer Vision to the Research of Object \\nRecognition in Plane Inspection,” J. Artif. Intell. Capsul. Networks , \\nvol. 2, no. 2, pp. 77 –99, 2020, doi: 10.36548/jaic n.2020.2.002.  \\n[11] Y. He, X. Li, and H. Nie, “A Moving Object Detection and \\nPredictive Control Algorithm Based on Deep Learning,” J. Phys. \\nConf. Ser. , vol. 2002, no. 1, 2021, doi: 10.1088/1742 -\\n6596/2002/1/012070.  \\n[12] S. Ren, C. Han, X. Yang, G. Han, and S . He, “TENet: Triple \\nExcitation Network for Video Salient Object Detection,” Lect. Notes \\nComput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes \\nBioinformatics) , vol. 12350 LNCS, pp. 212 –228, 2020, doi: \\n10.1007/978 -3-030-58558 -7_13.  \\n[13] X. Qin, Z. Zhang, C. Huang, M. Dehghan, O. R. Zaiane, and M. \\nJagersand, “U2 -Net: Going deeper with nested U -structure for salient \\nobject detection,” Pattern Recognit. , vol. 106, 2020, doi: \\n10.1016/j.patcog.2020.107404.  \\n[14] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R -CNN: Towards \\nReal-Time Object Detection with Region Proposal Networks,” IEEE \\nTrans. Pattern Anal. Mach. Intell. , vol. 39, no. 6, pp. 1137 –1149, \\n2017, doi: 10.1109/TPAMI.2016.2577031.  \\n[15] Z. Cai and N. Vasconcelos, “Cascade R -CNN: High quali ty object \\ndetection and instance segmentation,” IEEE Trans. Pattern Anal. \\nMach. Intell. , vol. 43, no. 5, pp. 1483 –1498, 2021, doi: \\n10.1109/TPAMI.2019.2956516.  \\n[16] S. Chen, X. Tan, B. Wang, and X. Hu, “Reverse attention for salient \\nobject detection,” Lect.  Notes Comput. Sci. (including Subser. Lect. \\nNotes Artif. Intell. Lect. Notes Bioinformatics) , vol. 11213 LNCS , pp. \\n236–252, 2018, doi: 10.1007/978 -3-030-01240 -3_15.  \\n 00.20.40.60.811.2\\n1 2 3 4 5 6 7\\nE POCHS  \\nSensitivity Specificity Precision F-Score\\n010203040506070\\nCPU(Mins) COLAB(Mins) GPU(Mins)EXECUTION TIME(mins)  Proceedings of the International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT 2023)\\nIEEE Xplore Part Number: CFP23CV1-ART; ISBN: 978-1-6654-7451-1\\n978-1-6654-7451-1/23/$31.00 ©2023 IEEE 361\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 18,2024 at 14:27:08 UTC from IEEE Xplore.  Restrictions apply. \", metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Detection of Crime Scene Objects using Deep \\nLearning Techniques \\n \\nNandhini T J  K Thinakaran  \\nDepartment of Computer Science and Engineering  Department of Computer Science and Engineering  \\nSaveetha School of Engineering  Saveetha School of Engineering  \\nSaveetha Institute of Medical and Technical Science ,  \\nChennai, India . Saveetha Institute of Medical and Technical Science,  \\nChennai, India.  \\nnandhinitj67@gmail.com  thinakarank.sse@saveetha.com  \\n \\nAbstract-  Research on the detection of objects at crime scenes has \\nflourished in the last two decades. Researchers have been \\nconcentrating on color pictures, where lighting is a crucial \\ncomponent, since this is one of the most pressing issues in \\ncomputer vision, with applications spanning surveillance, security, \\nmedicine, and more. However, night time monitoring is crucial \\nsince most security problems cannot be seen by the naked eye. \\nThat's why it's crucial to record a dark scene and identify the\", metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 0}),\n",
       " Document(page_content='things at a crime scene. Even when its dark out, infrared cameras \\nare indispensable. Both military and civilian sectors will benefit \\nfrom the use of such methods for night time navigation. On the \\nother hand, IR photographs have issues with poor resolution, \\nlighting effects, and other similar issues. Surveillance cameras \\nwith infrared (IR) imaging capabilities have been the focus of \\nmuch study and development in recent years. This research work \\nhas attempted to offer a good model for object recognition by \\nusing IR images obtained from crime scenes using Deep Learning. \\nThe model is tested in many scenarios including a central \\nprocessing unit (CPU), Google COLAB, and graphics processing \\nunit (GPU), and its performance is also tabulated. \\n \\nKeywords: Object detection, Deep Learning, COLA B, CNN, Crime \\nscene. \\n \\nI. INTRODUCTION \\nIn crime scene object detection, a wide range of computer \\nvision applications, including autonomous driving, cutting-', metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 0}),\n",
       " Document(page_content='edge driving assistance systems, robotic visions, augmented \\nreality, etc., make object identification a significant and active \\narea of research. The primary goal of object detection is to \\nlocate and categorize particular items in still photographs and \\nmoving pictures.[1][2]. The process of focusing on an object \\ninvolved in the vision process, such as visual tracking, human \\nre-identification, and semantic segmentation, is typically seen \\nas an essential step.[3] The semantic object detection technique \\nmakes use of several geometric patterns as proof to spot \\nintriguing objects in pictures or movies.[4] The patterns of \\nforms that display a comparable category of objects are used to \\ntrain the object recognition models to distinguish between \\ndistinct categories. However, because the characteristics of \\nbasic item forms, object positions, and angles of view vary \\nwidely, it is challenging for a system to reliably identify every', metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 0}),\n",
       " Document(page_content='appearance of an object.[5] Multiple object detection uses \\nsimilarities between the succession of photos or videos \\ndetermine the movement of things. Target objects are first \\nidentified in multiple object detection and the technique is then \\nfollowed to assess the itinerary of the items using the results of \\nthe detection. The journey of many objects is formed by \\nsemantic object detection with detection, which utilizes \\nassociated data from the existing track and fresh identification \\nfrom each frame.[6] Thus, a sequence of detections with distinct identities is produced as a result of data association. \\nRecognition of salient objects might be difficult when they \\nhave similar appearances. The moving objects are indicated for \\ndifferentiating and monitoring the different objects in this \\nsituation.[7] When using a single moving camera, the \\nmovement of the global camera, which cannot be seen, \\ncontaminates the discernible indications of motion. Due to the', metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 0}),\n",
       " Document(page_content='decreased characteristics of images, such as blurriness in \\nmotion and defocus on films, which results in inconsistent \\ncategorization for comparable objects, object detection \\npresented a challenge. Convolutional Neural Network (CNN), \\nfaster region-based CNN, spatial pyramid pooling network, \\nregion-based Fully CNN, You Only Look Once (YOLO), and \\nFeature Pyramid Network (FPN) are some of the deep learning \\nmodels that have been used to build the semantic object \\nidentification approach.[8] \\nThis study develops an algorithm to categorise different crime \\nscene photographs and to recognise different things in them. A \\nvideo that is composed of photographs from crime scenes is \\ntaken into account and divided into frames. The frame rate \\nrange for video is between 45 to 120 frames per second, or \\n7200 pictures per minute[9][10]. When processing any video, \\nthis step is frequently taken. A seven-layered convolutional \\nneural network is used to run the video after receiving the', metadata={'source': 'Detection_of_Crime_Scene_Objects_using_Deep_Learning_Techniques.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=50)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector Embedding And Vector Store\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents[:5],OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1ddaeaef790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decreased characteristics of images, such as blurriness in \\nmotion and defocus on films, which results in inconsistent \\ncategorization for comparable objects, object detection \\npresented a challenge. Convolutional Neural Network (CNN), \\nfaster region-based CNN, spatial pyramid pooling network, \\nregion-based Fully CNN, You Only Look Once (YOLO), and \\nFeature Pyramid Network (FPN) are some of the deep learning \\nmodels that have been used to build the semantic object \\nidentification approach.[8] \\nThis study develops an algorithm to categorise different crime \\nscene photographs and to recognise different things in them. A \\nvideo that is composed of photographs from crime scenes is \\ntaken into account and divided into frames. The frame rate \\nrange for video is between 45 to 120 frames per second, or \\n7200 pictures per minute[9][10]. When processing any video, \\nthis step is frequently taken. A seven-layered convolutional \\nneural network is used to run the video after receiving the'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query= \"An attention function can be described as mapping a query\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm  = Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt= ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following questions based only on the provided context.\n",
    "    Think step by step before providing a detailed answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001DDAEAEF790>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retreiver = db.as_retriever()\n",
    "retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retreival_chain = create_retrieval_chain(retreiver,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': 'speech.txt'})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retreival_chain.invoke({\"input\":\"The current algorithms effectively identify items in some labeled photos,\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
